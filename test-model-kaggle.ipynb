{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install azure-ai-textanalytics\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"id":"fKuozprj_wKy","execution":{"iopub.status.busy":"2024-11-13T13:18:44.954778Z","iopub.execute_input":"2024-11-13T13:18:44.955182Z","iopub.status.idle":"2024-11-13T13:22:44.247649Z","shell.execute_reply.started":"2024-11-13T13:18:44.955140Z","shell.execute_reply":"2024-11-13T13:22:44.246491Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from azure.core.credentials import AzureKeyCredential\nfrom azure.ai.textanalytics import TextAnalyticsClient\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-11-13T13:22:44.249735Z","iopub.execute_input":"2024-11-13T13:22:44.250096Z","iopub.status.idle":"2024-11-13T13:23:06.875636Z","shell.execute_reply.started":"2024-11-13T13:22:44.250058Z","shell.execute_reply":"2024-11-13T13:23:06.874617Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"}]},{"cell_type":"code","source":"def classify_text(query):\n    try:\n\n        ai_endpoint = 'https://myclassification51.cognitiveservices.azure.com/'\n        ai_key = '2AguPGY8KcIcngKeVUidX7JxS9DrkKbEvxpkVYatZUGuE0Nab5bbJQQJ99AKACYeBjFXJ3w3AAAaACOGYgZA'\n        project_name = 'ClassifyLab'\n        deployment_name = 'MyDeployment'\n\n        # Create client using endpoint and key\n        credential = AzureKeyCredential(ai_key)\n        ai_client = TextAnalyticsClient(endpoint=ai_endpoint, credential=credential)\n\n        # Prepare the query for classification\n        batchedDocuments = [query]\n\n        # Get Classification\n        operation = ai_client.begin_single_label_classify(\n            batchedDocuments,\n            project_name=project_name,\n            deployment_name=deployment_name\n        )\n\n        document_results = operation.result()\n\n        # Extract classification result\n        for classification_result in document_results:\n            if classification_result.kind == \"CustomDocumentClassification\":\n                classification = classification_result.classifications[0]\n                return classification.category, classification.confidence_score\n            elif classification_result.is_error:\n                return None, classification_result.error.message\n\n    except Exception as ex:\n        return None, str(ex)","metadata":{"id":"8_NR3R2O_3u_","execution":{"iopub.status.busy":"2024-11-13T13:23:06.888646Z","iopub.execute_input":"2024-11-13T13:23:06.889064Z","iopub.status.idle":"2024-11-13T13:23:06.927770Z","shell.execute_reply.started":"2024-11-13T13:23:06.889019Z","shell.execute_reply":"2024-11-13T13:23:06.926778Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\"mohamed517/Arabic-fine-Tuning-LLaMA-Model\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T13:23:27.088425Z","iopub.execute_input":"2024-11-13T13:23:27.089467Z","iopub.status.idle":"2024-11-13T13:24:01.469515Z","shell.execute_reply.started":"2024-11-13T13:23:27.089410Z","shell.execute_reply":"2024-11-13T13:24:01.468591Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.11.6: Fast Llama patching. Transformers = 4.46.2.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be530ebb5a5c4c99863db5450ef33360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c0d2c1e8f64e34908fb416aebfdf22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725b1ec7bcb44d439bea265f34c185b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06e4089fcab4209a9923f8cc73111aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667b67211afb48eabd4de838a925ff32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b826da9a0b1a4453af24ebe5dc40fcf4"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2024.11.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T13:24:05.999883Z","iopub.execute_input":"2024-11-13T13:24:06.000381Z","iopub.status.idle":"2024-11-13T13:24:06.006968Z","shell.execute_reply.started":"2024-11-13T13:24:06.000335Z","shell.execute_reply":"2024-11-13T13:24:06.005799Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def generation(question):\n\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n    category, confidence_score = classify_text(question)\n\n    context = f\"Ø§Ù†Øª Ù…Ø¹Ø§Ù„Ø¬ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‚ÙŠØ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆÙ…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø¯Ø¹Ù… Ø¹Ø§Ø·ÙÙŠ Ù…Ø¯Ø±ÙˆØ³ ÙˆÙ…Ø®ØµØµ Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù†Ø§Ø¡ Ø¹Ù„ÙŠ Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ Ø§Ù„Ø­Ø§Ù„ÙŠÙ‡ Ø³ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø¨Ø¹Ù†Ø§ÙŠÙ‡ Ø§Ù„ÙŠ Ù…Ø®Ø§ÙˆÙÙ‡Ù… ÙˆÙ…Ø´Ø§Ø¹Ø±Ù‡Ù… Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ù‡ Ø§Ù† Ø§Ù„Ø­Ø§Ù„Ù‡ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ Ù„Ù„Ù…Ø±ÙŠØ¶ Ù‡ÙŠ {category} Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù…Ù„Ø§Ø¡Ù…Ù‡ Ù„Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø°ÙŠ ÙŠÙ…Ø± Ø¨Ù‡ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø¨Ù…Ø®ØªÙ„Ù Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„Ø¹Ù„Ø§Ø¬ÙŠÙ‡ Ù„ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ù†ÙŠØ§Øª ÙˆÙ…Ø­Ø§Ø¯Ø«Ø§Øª Ø¯Ø§Ø¹Ù…Ù‡ Ø¨Ù„Ù‡Ø¬Ù‡ ÙˆØ¯ÙˆØ¯Ù‡ ÙˆÙ…ØªÙÙ‡Ù…Ù‡ ØªØ°ÙƒØ± Ø§Ù†Ùƒ Ù…ÙˆØ±Ø¯ Ù„Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ ÙˆØ§Ù„Ø§Ø±Ø´Ø§Ø¯ ÙˆÙ„Ø³Øª Ø¨Ø¯ÙŠÙ„Ø§ Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨Ø´Ø±ÙŠ ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±ÙŠØ¶ Ø¨ØµØ¯Ù‚ ÙˆØ§Ø­ØªØ±Ø§Ù… ÙˆÙƒÙ† Ù…Ø±Ù†Ø§ ÙÙŠ Ù…Ø­Ø§Ø¯Ø«Ø§ØªÙƒ Ù„ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ ÙˆØ­Ø§ÙˆÙ„ Ø§Ù„ØªØ®ÙÙŠÙ Ø¹Ù†Ù‡ Ø¨Ø·Ø±ÙŠÙ‚Ù‡ Ù„Ø·ÙŠÙÙ‡ ÙˆÙ…ØªÙˆØ§Ø²Ù†Ù‡\"\n\n    # Create the messages list with the context and user input\n    messages = [\n        {\"from\": \"system\", \"value\": context},\n        {\"from\": \"human\", \"value\": question},\n    ]\n\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True, # Must add for generation\n        return_tensors = \"pt\",\n    ).to(\"cuda\")\n\n    outputs = model.generate(input_ids = inputs, max_new_tokens = 400, use_cache = True)\n    model_answer = tokenizer.batch_decode(outputs)\n\n    return model_answer","metadata":{"id":"_eSFpXg5AGBe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b97a024-e72e-426e-b6be-cd6789c0ba71","execution":{"iopub.status.busy":"2024-11-13T13:24:09.533882Z","iopub.execute_input":"2024-11-13T13:24:09.534914Z","iopub.status.idle":"2024-11-13T13:24:09.542803Z","shell.execute_reply.started":"2024-11-13T13:24:09.534867Z","shell.execute_reply":"2024-11-13T13:24:09.541710Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def adjustPattern(model_answer):\n    \n    # Assuming model_answer is a list with the model's output string at the first index\n    model_answer_str = model_answer[0]\n\n    # Adjusted regex pattern to capture all relevant sections\n    pattern = re.compile(r'<\\|start_header_id\\|>(.*?)<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', re.DOTALL)\n\n    # Find all matches and store them in a structured format\n    sections = pattern.findall(model_answer_str)\n    output = {\"user\": [], \"bot\": []}\n\n    for role, content in sections:\n        role = role.strip().lower()  # Normalize the role for comparison\n        content = content.strip()  # Clean up the content\n        if role == 'system':\n            continue  # Skip the system role\n        elif role == 'user':\n            output[\"user\"].append(content)\n        elif role == 'assistant':\n            output[\"bot\"].append(content)\n\n    # Check for any direct assistant response after the user's input if not captured above\n    if 'assistant' not in [role.strip().lower() for role, _ in sections]:\n        assistant_start_index = model_answer_str.find('<|start_header_id|>assistant<|end_header_id|>')\n        if assistant_start_index != -1:\n            assistant_content = model_answer_str[assistant_start_index:].split('<|eot_id|>')[0]\n            output[\"bot\"].append(\n                assistant_content.replace('<|start_header_id|>', '').replace('<|end_header_id|>', '').strip()\n            )\n    \n    return output","metadata":{"id":"a0zVj3xoBqGX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f56ba1b0-90c4-4588-ed7e-c97eb2d5ecde","execution":{"iopub.status.busy":"2024-11-13T13:24:13.028423Z","iopub.execute_input":"2024-11-13T13:24:13.029114Z","iopub.status.idle":"2024-11-13T13:24:13.038733Z","shell.execute_reply.started":"2024-11-13T13:24:13.029068Z","shell.execute_reply":"2024-11-13T13:24:13.037743Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"question = \"Ø¹Ù„ÙŠ Ø·ÙˆÙ„ Ù…Ø¶Ø§ÙŠÙ‚Ù‡ ÙˆÙ†ÙØ³ÙŠØªÙŠ ØªØ¹Ø¨Ø§Ù†Ù‡ ÙˆØ¨Ø³ Ø§Ø¨ÙƒÙŠ Ù…Ø§ Ø§Ø¨ØºÙŠ Ø§Ù‚Ø§Ø¨Ù„ Ø§Ø­Ø¯ ÙˆØ¹Ù„ÙŠ Ø·ÙˆÙ„ Ø§Ø®Ø§ØµÙ… Ø§Ø·ÙØ§Ù„ÙŠ ÙˆØ§ØµØ±Ø® Ø¹Ù„ÙŠÙ‡Ù… ÙˆØ§Ø±ÙˆØ­ Ø§Ù„ØºØ±ÙÙ‡ Ù„ÙˆØ­Ø¯ÙŠ Ù…Ø¹ Ø§Ù„Ø¹Ù„Ù… Ø§Ù†Ù‡ ÙÙŠ Ø§Ù„Ù„ÙŠÙ„ Ø§Ø²Ø¹Ù„ Ø¹Ù„ÙŠ ØªØµØ±ÙØ§ØªÙŠ ÙÙ…Ø§ Ø§Ù„Ø­Ù„\"\n\nmodel_answer = generation(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T13:41:38.296066Z","iopub.execute_input":"2024-11-13T13:41:38.296522Z","iopub.status.idle":"2024-11-13T13:41:47.964041Z","shell.execute_reply.started":"2024-11-13T13:41:38.296479Z","shell.execute_reply":"2024-11-13T13:41:47.963132Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Call the function\nparsed_output = adjustPattern(model_answer)\n\n# Access and print each role's content\nfor role, contents in parsed_output.items():\n    print(f\"{role.capitalize()} response:\")\n    for content in contents:\n        print(f\"\\n{content}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T13:41:52.403776Z","iopub.execute_input":"2024-11-13T13:41:52.404195Z","iopub.status.idle":"2024-11-13T13:41:52.410131Z","shell.execute_reply.started":"2024-11-13T13:41:52.404154Z","shell.execute_reply":"2024-11-13T13:41:52.409195Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"User response:\n\nØ¹Ù„ÙŠ Ø·ÙˆÙ„ Ù…Ø¶Ø§ÙŠÙ‚Ù‡ ÙˆÙ†ÙØ³ÙŠØªÙŠ ØªØ¹Ø¨Ø§Ù†Ù‡ ÙˆØ¨Ø³ Ø§Ø¨ÙƒÙŠ Ù…Ø§ Ø§Ø¨ØºÙŠ Ø§Ù‚Ø§Ø¨Ù„ Ø§Ø­Ø¯ ÙˆØ¹Ù„ÙŠ Ø·ÙˆÙ„ Ø§Ø®Ø§ØµÙ… Ø§Ø·ÙØ§Ù„ÙŠ ÙˆØ§ØµØ±Ø® Ø¹Ù„ÙŠÙ‡Ù… ÙˆØ§Ø±ÙˆØ­ Ø§Ù„ØºØ±ÙÙ‡ Ù„ÙˆØ­Ø¯ÙŠ Ù…Ø¹ Ø§Ù„Ø¹Ù„Ù… Ø§Ù†Ù‡ ÙÙŠ Ø§Ù„Ù„ÙŠÙ„ Ø§Ø²Ø¹Ù„ Ø¹Ù„ÙŠ ØªØµØ±ÙØ§ØªÙŠ ÙÙ…Ø§ Ø§Ù„Ø­Ù„\n\nBot response:\n\nØ§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ø³Ø¯ÙŠ ÙŠØ­ØªØ§Ø¬Ø§Ù† Ø§Ù„ÙŠ ØªÙ‚ÙŠÙ… Ø·Ø¨ÙŠ Ù„Ù…Ø¹Ø±ÙÙ‡ Ø§Ù„Ø­Ø§Ù„Ù‡ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆØ¨Ø§Ù„ØªØ§ÙƒÙŠØ¯ ÙŠØ­ØªØ§Ø¬ Ø§Ù„ÙŠ Ø¹Ù„Ø§Ø¬ Ù†ÙØ³ÙŠ Ù…ÙƒØ«Ù ÙˆÙ…Ø®ØµØµ Ù„ÙƒÙ„ Ø­Ø§Ù„Ù‡\n\n","output_type":"stream"}]}]}