{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fKuozprj_wKy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install azure-ai-textanalytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "\n",
        "def classify_text(query):\n",
        "    try:\n",
        "\n",
        "        ai_endpoint = 'https://sentimentanalysis10.cognitiveservices.azure.com/'\n",
        "        ai_key = '4kyIh8KGdZYB9j9Yj71gT09yOE3x46rXQpfXilONXKm8CFL7ydK6JQQJ99AJACYeBjFXJ3w3AAAaACOGjCS5'\n",
        "        project_name = 'MentalHealth10'\n",
        "        deployment_name = 'MentalHealth'\n",
        "\n",
        "        # Create client using endpoint and key\n",
        "        credential = AzureKeyCredential(ai_key)\n",
        "        ai_client = TextAnalyticsClient(endpoint=ai_endpoint, credential=credential)\n",
        "\n",
        "        # Prepare the query for classification\n",
        "        batchedDocuments = [query]\n",
        "\n",
        "        # Get Classification\n",
        "        operation = ai_client.begin_single_label_classify(\n",
        "            batchedDocuments,\n",
        "            project_name=project_name,\n",
        "            deployment_name=deployment_name\n",
        "        )\n",
        "\n",
        "        document_results = operation.result()\n",
        "\n",
        "        # Extract classification result\n",
        "        for classification_result in document_results:\n",
        "            if classification_result.kind == \"CustomDocumentClassification\":\n",
        "                classification = classification_result.classifications[0]\n",
        "                return classification.category, classification.confidence_score\n",
        "            elif classification_result.is_error:\n",
        "                return None, classification_result.error.message\n",
        "\n",
        "    except Exception as ex:\n",
        "        return None, str(ex)"
      ],
      "metadata": {
        "id": "8_NR3R2O_3u_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "def generation(question,model,tokenizer):\n",
        "\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "    category, confidence_score = classify_text(question)\n",
        "\n",
        "    context = f\"انت معالج بالذكاء الاصطناعي قيد التدريب ومهمتك هي تقديم دعم عاطفي مدروس ومخصص لكل مستخدم بناء علي حالته النفسيه الحاليه ستقوم بالاستماع بعنايه الي مخاوفهم ومشاعرهم مع مراعاه ان الحاله النفسيه للمريض هي {category} قم بتقديم استجابات ملاءمه للوضع الذي يمر به استخدم معرفتك بمختلف المناهج العلاجيه لتقديم تقنيات ومحادثات داعمه بلهجه ودوده ومتفهمه تذكر انك مورد للدعم العاطفي والارشاد ولست بديلا عن المعالج البشري تعامل مع المريض بصدق واحترام وكن مرنا في محادثاتك لتتناسب مع حالته النفسيه وحاول التخفيف عنه بطريقه لطيفه ومتوازنه\"\n",
        "\n",
        "    # Create the messages list with the context and user input\n",
        "    messages = [\n",
        "        {\"from\": \"system\", \"value\": context},\n",
        "        {\"from\": \"human\", \"value\": question},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 300, use_cache = True)\n",
        "    model_answer = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    return model_answer\n"
      ],
      "metadata": {
        "id": "_eSFpXg5AGBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b97a024-e72e-426e-b6be-cd6789c0ba71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\"mohamed517/Arabic-fine-Tuning-LLaMA-Model\")"
      ],
      "metadata": {
        "id": "3VNNG1uHAcR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4d89d1-9824-4cab-e9e8-2d8427762c66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.10.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"اريد ان اشعر بالسعادة\""
      ],
      "metadata": {
        "id": "flJYCLq_Dm6g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_answer = generation(question,model,tokenizer)"
      ],
      "metadata": {
        "id": "0mIHiQvQB430"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Assuming model_answer is a list with the model's output string at the first index\n",
        "model_answer_str = model_answer[0]\n",
        "\n",
        "# Adjusted regex pattern to capture all relevant sections\n",
        "pattern = re.compile(r'<\\|start_header_id\\|>(.*?)<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', re.DOTALL)\n",
        "\n",
        "# Find all matches\n",
        "sections = pattern.findall(model_answer_str)\n",
        "\n",
        "# Print each section with clear labels\n",
        "for role, content in sections:\n",
        "    role = role.strip().lower()  # Normalize the role for comparison\n",
        "    content = content.strip()  # Clean up the content\n",
        "    if role == 'system':\n",
        "        continue  # Skip the system role\n",
        "    elif role == 'user':\n",
        "        print(f\"User:\\n\\n{content}\\n\")\n",
        "    elif role == 'assistant':\n",
        "        print(f\"Assistant:\\n\\n{content}\\n\")\n",
        "\n",
        "# Additionally, check for the assistant's response directly after user question if not captured above\n",
        "if 'assistant' not in [role.strip().lower() for role, _ in sections]:\n",
        "    assistant_start_index = model_answer_str.find('<|start_header_id|>assistant<|end_header_id|>')\n",
        "    if assistant_start_index != -1:\n",
        "        assistant_content = model_answer_str[assistant_start_index:].split('<|eot_id|>')[0]\n",
        "        print(f\"Model Answer:\\n\\n{assistant_content.replace('<|start_header_id|>', '').replace('<|end_header_id|>', '').strip()}\\n\")\n"
      ],
      "metadata": {
        "id": "a0zVj3xoBqGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56ba1b0-90c4-4588-ed7e-c97eb2d5ecde"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User:\n",
            "\n",
            "اريد ان اشعر بالسعادة\n",
            "\n",
            "Assistant:\n",
            "\n",
            "تعالج الاسباب النفسيه التي تءدي الي الاكتءاب\n",
            "\n"
          ]
        }
      ]
    }
  ]
}