{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fKuozprj_wKy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install azure-ai-textanalytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "\n",
        "def classify_text(query):\n",
        "    try:\n",
        "\n",
        "        ai_endpoint = 'https://sentimentanalysis10.cognitiveservices.azure.com/'\n",
        "        ai_key = '4kyIh8KGdZYB9j9Yj71gT09yOE3x46rXQpfXilONXKm8CFL7ydK6JQQJ99AJACYeBjFXJ3w3AAAaACOGjCS5'\n",
        "        project_name = 'MentalHealth10'\n",
        "        deployment_name = 'MentalHealth'\n",
        "\n",
        "        # Create client using endpoint and key\n",
        "        credential = AzureKeyCredential(ai_key)\n",
        "        ai_client = TextAnalyticsClient(endpoint=ai_endpoint, credential=credential)\n",
        "\n",
        "        # Prepare the query for classification\n",
        "        batchedDocuments = [query]\n",
        "\n",
        "        # Get Classification\n",
        "        operation = ai_client.begin_single_label_classify(\n",
        "            batchedDocuments,\n",
        "            project_name=project_name,\n",
        "            deployment_name=deployment_name\n",
        "        )\n",
        "\n",
        "        document_results = operation.result()\n",
        "\n",
        "        # Extract classification result\n",
        "        for classification_result in document_results:\n",
        "            if classification_result.kind == \"CustomDocumentClassification\":\n",
        "                classification = classification_result.classifications[0]\n",
        "                return classification.category, classification.confidence_score\n",
        "            elif classification_result.is_error:\n",
        "                return None, classification_result.error.message\n",
        "\n",
        "    except Exception as ex:\n",
        "        return None, str(ex)"
      ],
      "metadata": {
        "id": "8_NR3R2O_3u_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "def generation(question,model,tokenizer):\n",
        "\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "    category, confidence_score = classify_text(question)\n",
        "\n",
        "    context = f\"Ø§Ù†Øª Ù…Ø¹Ø§Ù„Ø¬ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‚ÙŠØ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆÙ…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø¯Ø¹Ù… Ø¹Ø§Ø·ÙÙŠ Ù…Ø¯Ø±ÙˆØ³ ÙˆÙ…Ø®ØµØµ Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù†Ø§Ø¡ Ø¹Ù„ÙŠ Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ Ø§Ù„Ø­Ø§Ù„ÙŠÙ‡ Ø³ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø¨Ø¹Ù†Ø§ÙŠÙ‡ Ø§Ù„ÙŠ Ù…Ø®Ø§ÙˆÙÙ‡Ù… ÙˆÙ…Ø´Ø§Ø¹Ø±Ù‡Ù… Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ù‡ Ø§Ù† Ø§Ù„Ø­Ø§Ù„Ù‡ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ Ù„Ù„Ù…Ø±ÙŠØ¶ Ù‡ÙŠ {category} Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù…Ù„Ø§Ø¡Ù…Ù‡ Ù„Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø°ÙŠ ÙŠÙ…Ø± Ø¨Ù‡ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø¨Ù…Ø®ØªÙ„Ù Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„Ø¹Ù„Ø§Ø¬ÙŠÙ‡ Ù„ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ù†ÙŠØ§Øª ÙˆÙ…Ø­Ø§Ø¯Ø«Ø§Øª Ø¯Ø§Ø¹Ù…Ù‡ Ø¨Ù„Ù‡Ø¬Ù‡ ÙˆØ¯ÙˆØ¯Ù‡ ÙˆÙ…ØªÙÙ‡Ù…Ù‡ ØªØ°ÙƒØ± Ø§Ù†Ùƒ Ù…ÙˆØ±Ø¯ Ù„Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ ÙˆØ§Ù„Ø§Ø±Ø´Ø§Ø¯ ÙˆÙ„Ø³Øª Ø¨Ø¯ÙŠÙ„Ø§ Ø¹Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨Ø´Ø±ÙŠ ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±ÙŠØ¶ Ø¨ØµØ¯Ù‚ ÙˆØ§Ø­ØªØ±Ø§Ù… ÙˆÙƒÙ† Ù…Ø±Ù†Ø§ ÙÙŠ Ù…Ø­Ø§Ø¯Ø«Ø§ØªÙƒ Ù„ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ ÙˆØ­Ø§ÙˆÙ„ Ø§Ù„ØªØ®ÙÙŠÙ Ø¹Ù†Ù‡ Ø¨Ø·Ø±ÙŠÙ‚Ù‡ Ù„Ø·ÙŠÙÙ‡ ÙˆÙ…ØªÙˆØ§Ø²Ù†Ù‡\"\n",
        "\n",
        "    # Create the messages list with the context and user input\n",
        "    messages = [\n",
        "        {\"from\": \"system\", \"value\": context},\n",
        "        {\"from\": \"human\", \"value\": question},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 300, use_cache = True)\n",
        "    model_answer = tokenizer.batch_decode(outputs)\n",
        "\n",
        "    return model_answer\n"
      ],
      "metadata": {
        "id": "_eSFpXg5AGBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b97a024-e72e-426e-b6be-cd6789c0ba71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\"mohamed517/Arabic-fine-Tuning-LLaMA-Model\")"
      ],
      "metadata": {
        "id": "3VNNG1uHAcR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4d89d1-9824-4cab-e9e8-2d8427762c66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.10.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Ø§Ø±ÙŠØ¯ Ø§Ù† Ø§Ø´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø©\""
      ],
      "metadata": {
        "id": "flJYCLq_Dm6g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_answer = generation(question,model,tokenizer)"
      ],
      "metadata": {
        "id": "0mIHiQvQB430"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Assuming model_answer is a list with the model's output string at the first index\n",
        "model_answer_str = model_answer[0]\n",
        "\n",
        "# Adjusted regex pattern to capture all relevant sections\n",
        "pattern = re.compile(r'<\\|start_header_id\\|>(.*?)<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', re.DOTALL)\n",
        "\n",
        "# Find all matches\n",
        "sections = pattern.findall(model_answer_str)\n",
        "\n",
        "# Print each section with clear labels\n",
        "for role, content in sections:\n",
        "    role = role.strip().lower()  # Normalize the role for comparison\n",
        "    content = content.strip()  # Clean up the content\n",
        "    if role == 'system':\n",
        "        continue  # Skip the system role\n",
        "    elif role == 'user':\n",
        "        print(f\"User:\\n\\n{content}\\n\")\n",
        "    elif role == 'assistant':\n",
        "        print(f\"Assistant:\\n\\n{content}\\n\")\n",
        "\n",
        "# Additionally, check for the assistant's response directly after user question if not captured above\n",
        "if 'assistant' not in [role.strip().lower() for role, _ in sections]:\n",
        "    assistant_start_index = model_answer_str.find('<|start_header_id|>assistant<|end_header_id|>')\n",
        "    if assistant_start_index != -1:\n",
        "        assistant_content = model_answer_str[assistant_start_index:].split('<|eot_id|>')[0]\n",
        "        print(f\"Model Answer:\\n\\n{assistant_content.replace('<|start_header_id|>', '').replace('<|end_header_id|>', '').strip()}\\n\")\n"
      ],
      "metadata": {
        "id": "a0zVj3xoBqGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56ba1b0-90c4-4588-ed7e-c97eb2d5ecde"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User:\n",
            "\n",
            "Ø§Ø±ÙŠØ¯ Ø§Ù† Ø§Ø´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø©\n",
            "\n",
            "Assistant:\n",
            "\n",
            "ØªØ¹Ø§Ù„Ø¬ Ø§Ù„Ø§Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù†ÙØ³ÙŠÙ‡ Ø§Ù„ØªÙŠ ØªØ¡Ø¯ÙŠ Ø§Ù„ÙŠ Ø§Ù„Ø§ÙƒØªØ¡Ø§Ø¨\n",
            "\n"
          ]
        }
      ]
    }
  ]
}